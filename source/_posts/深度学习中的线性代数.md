---
title: 深度学习中的线性代数
date: 2024-09-29 22:29:57
tags:
  - math
  - linear algebra
  - deep learning
categories: math
---

<!-- toc -->

# 线性代数

真正去学发现深度学习中用到的线代知识其实不多，基础的矩阵运算，像矩阵乘，转置之类我是懂的。就是一些正交矩阵，正定矩阵的概念不清楚，容易混淆。然后是不理解为什么要学特征值分解（）就这些东西，加上没见过的奇异值分解，就已经是全部了。

~~原来我也能学会数学啊。~~

## 符号与表示

- 向量在本书《深度学习》中用小写的黑体字母表示，如 $\mathrm{x}$，矩阵用大写黑体字母表示，如 $\mathrm{A}$。但是在本文中，由于 $\LaTeX$ 显示不同，我们使用 $\vec{x}$ 来表示向量。
- 在需要区分行向量和列向量的语境中，$\mathrm{x}$ 通常指列向量，行向量用 $\mathrm{x^T}$ 表示。有时为了在行间表示列向量，会写为 $\mathrm{x} = [x_1, x_2, \dots, x_n]^T$

## 理解向量和矩阵

### 向量

定义一个向量 $\vec{x}\in\mathbb{R}^n$，也就是一个 n 维向量 $\vec{x}$。

- 理解 1：向量很容易理解为在一个 n 维空间中的一个点。比如 $n=2$，向量 $\vec{x}$ 就是平面空间的一个点。

- 理解 2：同时，向量也可以理解为列数为 1 的矩阵。

### 矩阵

定义一个矩阵 $\mathrm{A}\in\mathbb{R}^{m\times n}$，即一个 m 行 n 列的矩阵。

- 理解 1：矩阵可以很容易地理解为一个二维数组，由第 $i$ 行第 $j$ 列确定每个元素。
- 理解 2：多个列向量的组合，$\mathrm{A}=\{\vec{v_{1}},\vec{v_2}\dots,\vec{v_n}\}$。

## 由一个式子出发

$$
\begin{align}
\mathrm{A}\vec{x}=\vec{b}
\end{align}
$$

其中 $\mathrm{A}\in\mathbb{R}^{m\times n}$ 是已知矩阵，$\vec{b}\in\mathbb{R}^m$ 是已知向量，$\vec{x}\in\mathbb{R}^n$ 是我们要求出的向量。

- 理解 1：矩阵 $\mathrm{A}$ 和向量 $\vec{x}$ 相乘，可以理解为一个 m 行 n 列的矩阵和一个 n 行 1 列的矩阵进行矩阵乘，得到一个 m 行 1 列的矩阵，也就是向量 $\vec{b}$。

- 理解 2：也可以理解为向量 $\vec{x}$ 中的每个**标量**元素 $x_i$，与一组**列向量** $\mathrm{A}=\{\vec{v_{1}},\dots,\vec{v_n}\}$ 中对应的列向量 $\vec{v_i}$ 进行标量乘，得到的 n 个列向量再在列维度进行求和，变为一个列向量 $\vec{b}$。

根据理解 2，我们可以把 $\mathrm{A}$ 的列向量看作从原点出发的不同方向，向量 $\vec{x}$ 的每个元素 $x_i$表示我们应该沿着对应的方向 $\vec{v_i}$ 走多远（进行标量乘），所有方向都走过之后（列维度求和），我们希望能到达位置 $\vec{b}$ （这里应该把 $\vec{b}$ 看作一个点）。

$$
\begin{align}
\mathrm{A}\vec{x}=\sum_{i}{x_i\vec{v_i}}=\vec{b}
\end{align}
$$

这种由一组向量分别乘以对应标量系数后沿列求和的操作（即 $\mathrm{A}\vec{x}$），称为**线性组合**（linear combination）。

一组向量的**生成子空间**（span），是原始向量线性组合后能抵达的点的集合（即 $\vec{b}$ 的集合）。矩阵 $\mathrm{A}$ 的生成子空间，被称为 $\mathrm{A}$ 的**列空间**（column space）或者 $\mathrm{A}$ 的**值域**（range）。

确定 $\mathrm{A}\vec{x}=\vec{b}$ 是否有解，相当于确定 $\vec{b}$ 是否在 $\mathrm{A}$ 的列向量的生成子空间中（或称为 $\mathrm{A}$ 的列空间/值域）。

### 矩阵逆

回顾一下 $\mathrm{A}\vec{x}=\vec{b}$，要求出所有的 $\vec{x}$，我们其实有一个简单的办法——使用逆矩阵。

<div>
$$
\begin{align}
\mathrm{A}\vec{x}&=\vec{b} \\
\mathrm{A^{-1}A}\vec{x}&=\mathrm{A^{-1}}\vec{b} \\
\vec{x}&=\mathrm{A^{-1}}\vec{b}
\end{align}
$$
</div>

因此我们的任务就变成了求解逆矩阵。

但是逆矩阵并不是任何时候都存在的，如果逆矩阵 $\mathrm{A^{-1}}$ 存在，那么 $\mathrm{A}\vec{x}=\vec{b}$ 对于任意向量 $\vec{b}$ **有且仅有一个解**。

解的情况有三种：没有解（即无法达到的点），有一个解（即有唯一对应的线性组合），有无限个解（有无限个对应的线性组合）。

> 为什么没有多于一个但少于无限多个解的情况？
>
> 因为如果 $\vec{x}$ 和 $\vec{y}$ 都是某方程组的解，那么
>
> $$
> \vec{z}=\alpha\vec{x}+(1-\alpha)\vec{y}
> $$
>
> （其中 $\alpha$ 取任意实数）也是该方程组的解。

- 要保证对于任意向量 $\vec{b}\in\mathbb{R}^m$ 有解，要求 $\mathrm{A}$ 的列空间构成整个 $\mathbb{R}^m$，即至少有 $m$ 个线性无关的向量，这要求 $n\ge m$。

- 要保证对于任意向量 $\vec{b}\in\mathbb{R}^m$ 至多只有 1 个解。需要矩阵 $\mathrm{A}$ 中的每个列向量都线性不相关（因为一旦线性相关，就会让有的 $\vec{b}$ 有无数个解）。如果列数 n 多于行数 m，就一定会有一个列向量是其他列向量的线性组合，因此矩阵的 $n\le m$。

综上两点，要使逆矩阵 $\mathrm{A^{-1}}$ 存在，矩阵 $\mathrm{A}$ 需要为一个方阵，即 $m=n$，且 n 个列向量都线性无关。这样的方阵称为非奇异矩阵。相对地，一个线性相关的方阵被称为**奇异的**（singular）。

如果矩阵 $\mathrm{A}$ 不是方阵，或是一个奇异的方阵，方程 $\mathrm{A}\vec{x}=\vec{b}$ 仍有可能有解，但是我们不能用逆矩阵求解。

### 范数

比较熟悉的范数我就直接给出公式，不进行解释了。

- $L^p$ 范数
  <div>
  $$
  \|\vec{x}\|_p=\Big(\sum_i{|x_i|^p}\Big)^{\frac{1}{p}}
  $$
  </div>

  其中 $p\in\mathbb{R},p>1$。

  当 $p=2$ 时，$L^{2}$ 称为欧几里得范数，表示从原点出发到 $\vec{x}$ 的欧几里得距离。由于使用频繁，经常简化表示为 $\|\vec{x}\|$。

  有时我们也用**平方 $L^2$ 范数**来衡量向量大小，少了开根号步骤，可以直接用点积 $\vec{x}^T\vec{x}$ 计算。

- $L^{\infty}$ 范数，也称最大范数（max norm），表示向量中具有最大值的元素的绝对值。
  <div>
  $$
  \|\vec{x}\|_{\infty}=\max_i|x_i|
  $$
  </div>

- **Frobenius 范数**，即矩阵的 $L^2$ 范数。
  <div>
  $$
  \|\mathrm{A}\|_F=\sqrt{\sum_{i,j}\mathrm{A}^2_{i,j}}
  $$
  </div>

  值得一提的是，如果引入矩阵的**迹**（Trace），即矩阵对角元素的和：
  <div>
  $$
  \mathbf{Tr}(\mathrm{A})=\sum_i{\mathrm{A}_{i,j}}
  $$
  </div>

  我们可以把矩阵 $\mathrm{A}$ 的 Frobenius 范数表示成：
  <div>
  $$
  \|\mathrm{A}\|_F=\sqrt{\mathbf{Tr}(\mathrm{A}\mathrm{A}^T)}
  $$
  </div>

  推导过程如下：两个矩阵相乘可以看成是左矩阵第 i 行的行向量和右矩阵第 j 列的列向量的点积，得到的标量即为结果的 <span> $\mathrm{A_{i,j}}$ </span>元素。
  
  那么<span>$\mathrm{A}\mathrm{A}^T$</span>对角线上的元素 $\mathrm{A_{i,i}}$，就相当于第 i 个行向量和第 i 个列向量（实际上就是同一个行向量的转置）的点积，也就是该行向量每个元素的平方之和（或者说平方 $L^2$ 范数），将每一行元素的平方和进行相加，得到的结果就相当于矩阵每个元素的平方之和。

### 特殊矩阵和向量

- 对角矩阵（diagonal matrix）：只在主对角线上含有非零元素，其他位置都是0。我们用 $diag(a)$对角矩阵计算乘法很高效

# 概率论

优化问题，可以把通过给定的输入 $\mathrm{X}$，让观测到真值 $\mathrm{y}$ 的概率最大，这个概率 $P(\mathrm{y}|\mathrm{X})$ 也称为似然（likelihood）。
