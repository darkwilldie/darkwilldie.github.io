---
title: 机器学习
date: 2024-12-14 20:34:22
tags: 
categories: ML
---

<!-- toc -->

## 监督学习

### 感知机

感知机是二分类的线性分类模型，输出为+1或-1。目的是找出一个超平面，将正负样本分开。用函数表示为

$$
f(x) = sign(w \cdot x + b)
$$

其中$w$是权重，$b$是偏置。$w$也是超平面的法向量，$b$是超平面的截距。

> 为什么$w$是超平面的法向量？
>
> 取任意两个点$x_1$和$x_2$，相减有$w \cdot (x_1 - x_2) = 0$，显然$(x_1 - x_2)\neq0$且$w\neq0$，所以$w$与$x_1 - x_2$垂直，即垂直于超平面。

#### 损失函数

$$
L(w, b) = -\sum_{x_i \in M} y_i (w \cdot x_i + b)
$$

$M$是误分类点的集合。误分类点满足$y_i (w \cdot x_i + b) \leq 0$。加上负号并求和，总损失 $\gt0$。

#### 梯度

损失$L(w, b)$对$w$和$b$的梯度为
$$
\nabla_w L(w, b) = -\sum_{x_i \in M} y_i x_i
$$

$$
\nabla_b L(w, b) = -\sum_{x_i \in M} y_i
$$

#### 算法流程总结

1. 初始化$w_0=0$和$b_0=0$，学习率$\eta$
2. 选取一个误分类点$(x_i, y_i)$
3. 更新$w$和$b$
    $$
    w \leftarrow w + \eta y_i x_i
    $$

    $$
    b \leftarrow b + \eta y_i
    $$
    注意$x_i\in\mathbb{R}^n$，$y_i\in\{-1, 1\}$, 所以$w\in\mathbb{R}^n$，$b\in\mathbb{R}$。
4. 重复2和3，直到没有误分类点

### k近邻（KNN）

计算距离，选择最近的k个点，投票决定类别。

#### 距离

闵可夫斯基距离

$$
d(x, y) = \left( \sum_{i=1}^n |x_i - y_i|^p \right)^{\frac{1}{p}}
$$

当$p=1$时，为曼哈顿距离，又叫城市街区距离，即各坐标距离的绝对值之和。

$$
d(x, y) = \sum_{i=1}^n |x_i - y_i|
$$

当$p=2$时，为欧氏距离，即各坐标距离的平方和。

$$
d(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}
$$

当$p=\infty$时，为切比雪夫距离，即各坐标距离的最大值。

$$
d(x, y) = \max_i |x_i - y_i|
$$

#### kd树

kd树是一种二叉树，用于快速搜索k近邻。

#### 构建

1. 选择一个维度，将数据集按该维度排序，取中位数作为根节点。
2. 将数据集按该维度分成两部分，分别作为左右子树。
3. 重复1和2，直到所有数据都被处理。

#### 搜索

1. 找出包含目标点的叶节点。
2. 以该叶节点为当前最近点。
3. 从该叶节点开始，递归向上回溯，如果该节点到目标点的距离小于当前最近点到目标点的距离，则更新当前最近点。
4. 检查另一子树，如果另一子树到目标点的距离小于当前最近点到目标点的距离（即与目标点到最近点的圆相交），则递归进入另一子树搜索。
5. 重复3和4，直到回溯到根节点。
6. 返回当前最近点。

### 决策树

if-then规则的集合。

#### 最小二乘回归树生成

1. 选择最优切分变量$j$和切分点$s$，使得
    $$
    \min_{j, s} \left[ \min_{c_1} \sum_{x_i \in R_1(j, s)} (y_i - c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j, s)} (y_i - c_2)^2 \right]
    $$
2. 用选定的$(j, s)$划分区域，并决定相应的输出值。
3. 继续对两个子区域调用步骤1和2，直到满足停止条件。
4. 将输入空间划分为$M$个区域$R_1, R_2, \cdots, R_M$，生成决策树。

## 无监督学习

### 聚类

#### k均值

1. 随机选择k个点作为初始聚类中心。
2. 将每个点分配到最近的聚类中心。
3. 更新聚类中心为每个聚类中所有点的均值。
4. 重复2和3，直到聚类中心不再变化。

### 奇异值分解

奇异值分解（SVD）是一种矩阵分解方法，将一个矩阵分解为三个矩阵的乘积。

$$
A = U \Sigma V^T
$$

其中$U$和$V$是正交矩阵，$\Sigma$是对角矩阵。
