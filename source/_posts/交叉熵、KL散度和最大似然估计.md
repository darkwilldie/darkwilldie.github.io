---
title: 交叉熵、KL散度和最大似然估计
date: 2024-12-24 10:45:01
tags:
  - cross-entropy
  - kl-divergence
  - maximum-likelihood
categories: math
---
<!-- toc -->

## 基本概念

1. KL散度(Kullback-Leibler Divergence)
   - 用于衡量两个概率分布P和Q的差异
   - 定义：$D_{KL}(P||Q) = \sum_i P(i)\log\frac{P(i)}{Q(i)}$
   - 特点：非对称、非负，当且仅当P=Q时为0

2. 交叉熵(Cross Entropy)
   - 用于衡量两个概率分布的差异
   - 定义：$H(P,Q) = -\sum_i P(i)\log Q(i)$
   - 在深度学习中常用作损失函数

3. 最大似然估计(Maximum Likelihood Estimation)
   - 用于估计模型参数
   - 目标是最大化观测数据的概率
   - 对数似然：$\log L(\theta) = \sum_i \log P(x_i|\theta)$

## 三者关系

1. KL散度与交叉熵
   $$D_{KL}(P||Q) = H(P,Q) - H(P)$$
   其中$H(P)$是P的熵，在训练过程中是常数。因此：
   - 最小化KL散度 等价于 最小化交叉熵
   - 这就是为什么深度学习中常用交叉熵作为损失函数

2. 最大似然与交叉熵
   - **在真实标签为独热编码时**，最大化对数似然 等价于 最小化真实分布P与模型分布Q之间的交叉熵
   - 证明：  
     对于数据集 $X = \{x_1, x_2, ..., x_n\}$，真实标签为独热编码，即 $y_i \in \{0, 1\}$，且 $y_j = 1$  
     设预测概率分布为Q，则对数似然为：
     $$\log L(\theta) = \sum_{i=1}^n \log Q(y_i|x_i,\theta) = \sum_{i=1}^n y_j \log Q(y_i|x_i,\theta)$$
     交叉熵为：
     $$L = -\sum_{i=1}^n \sum_{k=1}^K y_{i,k} \log Q(y_{i,k}|x_i,\theta)$$
     - $y_{i,k}$ 是第$i$个样本的第$k$个类别的独热编码  

     对于单个样本
     - 如果真实标签是第$k$个类别，则只有第$j$类的交叉熵会被保留：
     $$L = -\sum_{i=1}^n \log Q(y_j|x_i,\theta)$$
     因此，最大化对数似然 等价于 最小化交叉熵

## 在深度学习中的应用

1. 分类任务
   - 使用交叉熵损失函数
   - 本质是最小化预测分布与真实分布的KL散度
   - 等价于对模型进行最大似然估计

2. 生成模型
   - VAE中使用KL散度作为正则项
   - GAN中通过最小化JS散度（KL散度的对称版本）
   - 都可以解释为最大似然估计的变体

## 选择建议

1. 分类问题
   - 直接使用交叉熵损失
   - 计算简单，梯度稳定

2. 生成模型
   - 需要考虑分布的完整性，使用KL散度
   - 可能需要配合其他技巧处理数值稳定性

3. 参数估计
   - 明确的概率模型，使用最大似然估计
   - 可以结合正则化避免过拟合


