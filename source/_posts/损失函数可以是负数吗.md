---
title: 损失函数可以是负数吗
date: 2024-09-10 14:19:56
tags:
  - deep-learning
  - loss-function
categories: deep-learning
---

今天尝试改代码的时候不得不面对这样一个问题：代码中计算的损失函数，是相关性（正数，越大越优）直接取负的结果。

具体来说：

```python
    # 损失函数
    def _loss_fn(self, S_batch, G_with_image, M_block):
        ...
        gv_term = self.lambda_g1 * calculate_correlations(G_pred, G_with_image)
        return -gv_term
    # 求损失并反向传播
    loss = self._loss_fn(S_batch, self.spot_feature, M_block)
    loss.backward()
```

因此引起了一个疑惑，这里的 loss 是一个负数，负数怎么进行优化呢？我在之前自学过的一些深度学习的基础知识，一直认为 loss 应该是一个正数，而反向传播后优化器进行 step 这个步骤，应该是让 loss 更接近 0。

经过搜索，我找到了一个解答了我所有问题的[回答](https://discuss.pytorch.org/t/what-happens-when-loss-are-negative/47883)。由于问题中有很多事实错误，因此不翻译，只翻译回答部分。

> 普通的梯度下降的优化步骤是这样的
>
> 新的 loss = 旧的 loss - 学习率 \* 梯度
>
> 梯度下降（并且所有我见过的常见的优化函数都是）寻求最小化 loss，并不在乎最小值是一个很大的正数，一个接近 0 的值，还是一个很大的负数。它寻求的只是让 loss 更小（也就是代数上更负）。
>
> 你可以把你的 loss 替换成
>
> 优化后的 loss = 原来的 loss - 2 \* Pi
>
> 然后你会得到相同的训练结果和模型表现（除了 loss 中的所有值都会减小 2 \* Pi）
> 确实我们经常会用**当模型对训练数据的拟合完美时就会等于 0 的损失函数**，但优化算法并不在乎这个，它们只是让损失函数在代数上更负，而不是接近于 0。

